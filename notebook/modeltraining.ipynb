{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc88287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, userdf, useroutputdf, inputLayer, hiddenLayer, outputLayer, learningRate=0.001, hyperparam=0.01):\n",
    "        self.inputLayer = inputLayer\n",
    "        self.hiddenLayer = hiddenLayer\n",
    "        self.outputLayer = outputLayer\n",
    "        self.learningRate = learningRate\n",
    "        self.hyperparam = hyperparam\n",
    "        self.df = userdf\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.netOutputs  = []\n",
    "        self.postactivationOutputs = []\n",
    "        self.predictions = []\n",
    "        self.actualOutputs = []\n",
    "        self.outdf = useroutputdf\n",
    "\n",
    "    def leakyRELU(self, arr):\n",
    "        return np.array([i if i>0 else self.hyperparam*i for i in arr])\n",
    "    \n",
    "    \n",
    "    def extractInput(self, i):\n",
    "        \n",
    "        arr = np.zeros(10)\n",
    "        arr[self.outdf[i]] = 1\n",
    "        return self.df[i].flatten(), arr\n",
    "    \n",
    "    def softmax(self, arr):\n",
    "        exps = np.exp(arr - np.max(arr))  \n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    def backPropogation(self, pred, actual, inputs):\n",
    "            \n",
    "            dl_by_dzfinal = pred - actual.flatten()\n",
    "\n",
    "            dl_by_dh = self.weights[1].T @ dl_by_dzfinal\n",
    "\n",
    "            leaky_derviative_firstLayr = np.array([1 if val > 0 else self.hyperparam for val in self.netOutputs[0]])\n",
    "\n",
    "            dl_by_dzh = dl_by_dh * leaky_derviative_firstLayr\n",
    "\n",
    "            grad_w1 = np.outer(dl_by_dzh, inputs)\n",
    "            grad_b1 = dl_by_dzh\n",
    "\n",
    "            grad_w2 = np.outer(dl_by_dzfinal, self.postactivationOutputs[0])\n",
    "            grad_b2 = dl_by_dzfinal\n",
    "\n",
    "            self.weights[0] -= self.learningRate * grad_w1\n",
    "            self.biases[0] -= self.learningRate * grad_b1\n",
    "            self.weights[1] -= self.learningRate * grad_w2\n",
    "            self.biases[1] -= self.learningRate * grad_b2\n",
    "\n",
    "    def forwardBackwardPass(self, i):\n",
    "        self.netOutputs = []\n",
    "        self.postactivationOutputs = []\n",
    "        inp = self.extractInput(i=i)\n",
    "\n",
    "        output = self.feedForward(inp[0])\n",
    "\n",
    "        pred = self.softmax(output)\n",
    "        self.predictions.append(np.argmax(pred))\n",
    "\n",
    "        print(f'pred: {pred}, actual: {inp[1]}')\n",
    "        self.backPropogation(pred=pred, actual=inp[1], inputs=inp[0])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(len(self.df)):\n",
    "            \n",
    "                \n",
    "  \n",
    "            \n",
    "            self.forwardBackwardPass(i=i)\n",
    "\n",
    "    def feedForward(self, inputVal):\n",
    "\n",
    "        r1 = inputVal @self.weights[0].T + self.biases[0]\n",
    "        a1 = self.leakyRELU(r1)\n",
    "\n",
    "        self.netOutputs.append(r1)\n",
    "        self.postactivationOutputs.append(a1)\n",
    "\n",
    "        r2 = a1 @ self.weights[1].T + self.biases[1]\n",
    "\n",
    "\n",
    "        \n",
    "        self.netOutputs.append(r2)\n",
    "\n",
    "\n",
    "        return r2\n",
    "\n",
    "    def predict(self, inp):\n",
    "      \n",
    "      r1 = inp @self.weights[0].T + self.biases[0]\n",
    "      a1 = self.leakyRELU(r1)\n",
    "      r2 = a1 @ self.weights[1].T + self.biases[1]\n",
    "\n",
    "      print(r2, list(r2).index(max(list(r2))))\n",
    "\n",
    "    def NN_init(self):\n",
    "        self.predictions = []\n",
    "        self.actualOutputs = []\n",
    "\n",
    "        w1 = np.random.rand(self.hiddenLayer, self.inputLayer)  * 0.01\n",
    "        w2 = np.random.rand(self.outputLayer, self.hiddenLayer) * 0.01\n",
    "\n",
    "        self.weights.append(w1)\n",
    "        self.weights.append(w2)\n",
    "\n",
    "        z1 = np.zeros(shape=(self.hiddenLayer))\n",
    "        z2 = np.zeros(shape=(self.outputLayer))\n",
    "        self.biases.append(z1)\n",
    "        self.biases.append(z2)\n",
    "\n",
    "NN = NeuralNetwork(x_train, y_train, 784, 1000, 10)\n",
    "NN.NN_init()\n",
    "\n",
    "NN.learn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f11d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, NN.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c396b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_weights = NN.weights\n",
    "good_biases = NN.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d3989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
